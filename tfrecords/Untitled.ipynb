{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stanley0/anaconda3/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'file_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-fc0ac9131ab5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \"\"\"\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_record_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'file_name' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\"\"\"Obtaining total number of records from .tfrecords file in Tensorflow?\n",
    "\n",
    "No it is not possible. TFRecord does not store any metadata about the data being stored inside.\n",
    "This file:\n",
    " represents a sequence of (binary) strings. The format is not random access, so it is suitable for streaming \n",
    " large amounts of data but not suitable if fast sharding or other non-sequential access is desired.\n",
    "\n",
    "If you want, you can store this metadata manually or use a record_iterator to get the number (you will need\n",
    "to iterate through all the records that you have:)\n",
    "\n",
    "\"\"\"\n",
    "sum(1 for _ in tf.python_io.tf_record_iterator(file_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"To count the number of records, you should be able to use tf.python_io.tf_record_iterator.\n",
    "\n",
    "        To just keep track of the model training, tensorboard comes in handy.\n",
    "\"\"\"\n",
    "c = 0\n",
    "for fn in tf_records_filenames:\n",
    "    for record in tf.python_io.tf_record_iterator(fn):\n",
    "        c += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFrecords Guide : http://warmspringwinds.github.io/tensorflow/tf-slim/2016/12/21/tfrecords-guide/\n",
    "# Getting raw data bytes in numpy\n",
    "import numy as np\n",
    "import skimage.io as io\n",
    "\n",
    "cat_img = io.imread('cat.jpg')\n",
    "io.imshow(cat_img)\n",
    "\n",
    "# Let's convert the picture into string representation\n",
    "# using the ndarray.tostring() function\n",
    "cat_string = cat_img.tostring()\n",
    "\n",
    "# Now let's convert the string back to the image\n",
    "# Important: the dtype should be specified\n",
    "# otherwise the reconstruction will be errorness\n",
    "# Reconstruction is 1d, so we need sizes of image to fully reconstruct it\n",
    "reconstructed_cat_1d = np.fromstring(cat_string, dtype=np.uint8)\n",
    "\n",
    "# Here we reshape the 1d representation\n",
    "# This is the why we need to store the sizes of image\n",
    "# along with its serialized representation.\n",
    "reconstructed_cat_img = reconstructed_cat_1d.reshape(cat_img.shape)\n",
    "\n",
    "# Let's check if we got everything right and compare\n",
    "# reconstructed array to the original one.\n",
    "np.allclose(cat_img, reconstructed_cat_img)\n",
    "\n",
    "\n",
    "# Get some image/annotation pairs for example \n",
    "filename_pairs = [\n",
    "('/home/dpakhom1/tf_projects/segmentation/VOCdevkit/VOCdevkit/VOC2012/JPEGImages/2007_000032.jpg',\n",
    "'/home/dpakhom1/tf_projects/segmentation/VOCdevkit/VOCdevkit/VOC2012/SegmentationClass/2007_000032.png'),\n",
    "('/home/dpakhom1/tf_projects/segmentation/VOCdevkit/VOCdevkit/VOC2012/JPEGImages/2007_000039.jpg',\n",
    "'/home/dpakhom1/tf_projects/segmentation/VOCdevkit/VOCdevkit/VOC2012/SegmentationClass/2007_000039.png'),\n",
    "('/home/dpakhom1/tf_projects/segmentation/VOCdevkit/VOCdevkit/VOC2012/JPEGImages/2007_000063.jpg',\n",
    "'/home/dpakhom1/tf_projects/segmentation/VOCdevkit/VOCdevkit/VOC2012/SegmentationClass/2007_000063.png')\n",
    "]\n",
    "\n",
    "\"\"\"Important: We are using PIL to read .png file later.\n",
    "This was done on purpose to read indexed png files in a special way\n",
    "-- only indexes and not map the indexes to actual rgb values. This is specific \n",
    "to PASCAL VOC dataset data. If you don't want this type of behavior \n",
    "consider using skimage.io.imread()\n",
    "\"\"\"\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import tensorflow as tf\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "tfrecords_filename = 'pascal_voc_segmentation.tfrecords'\n",
    "\n",
    "writer = tf.python_io.TFRecordWriter(tfrecords_filename)\n",
    "\n",
    "# Let's collect the real images to later on compare to the reconstructed ones\n",
    "original_images = []\n",
    "\n",
    "for img_path, annotation_path in filename_pairs:\n",
    "    img = np.array(Image.open(img_path))\n",
    "    annotation = np.array(Image.open(annotation_path))\n",
    "    \n",
    "    # The reason to store image sizes was demonstrated\n",
    "    # in the previous example -- we have to know sizes of images\n",
    "    # to later read raw serialized string, convert to 1d array and convert to\n",
    "    # respective shape that image used to have.\n",
    "    \n",
    "    height = img.shape[0]\n",
    "    width = img.shape[1]\n",
    "    \n",
    "    # Put in the original images into array\n",
    "    # Just for future check for correctness\n",
    "    original_images.append((img, annotation))\n",
    "    \n",
    "    img_raw = img.tostring()\n",
    "    annotation_raw = annotation.tostring()\n",
    "    \n",
    "    feature = {\n",
    "         'height': _int64_feature(height),\n",
    "        'width': _int64_feature(width),\n",
    "        'image_raw': _bytes_feature(img_raw),\n",
    "        'mask_raw': _bytes_feature(annotation_raw)\n",
    "    }\n",
    "    example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    \n",
    "    writer.write(example.SerializeToString())\n",
    "    \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_images = []\n",
    "\n",
    "record_iterator = tf.python_io.tf_record_iterator(path=tfrecords_filename)\n",
    "\n",
    "for string_record in record_iterator:\n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(string_record)\n",
    "    \n",
    "    height = int(example.features.feature['height']\n",
    "                    .in64_list\n",
    "                    .value[0])\n",
    "    width = int(example.features.feature['width']\n",
    "                    .in64_list\n",
    "                    .value[0])\n",
    "    img_string = (example.features.feature['image_raw']\n",
    "                    .bytes_list\n",
    "                    .value[0])\n",
    "    \n",
    "    annotation_string = (example.features.feature['mask_raw']\n",
    "                    .bytes_list\n",
    "                    .value[0])\n",
    "    \n",
    "    img_1d = np.fromstring(img_string, dtype=np.uint8)\n",
    "    reconstructed_img = img_1d.reshape((height, width, -1))\n",
    "    \n",
    "    annotation_1d = np.fromstring(annotation_string, dtype=np.uint8)\n",
    "    \n",
    "    # Annotations don't have depth (3rd dimension)\n",
    "    reconstructed_annotation = annotation_1d.reshape((height, width))\n",
    "    reconstructed_images.append((reconstructed_img, reconstructed_annotation))\n",
    "    \n",
    "# Let's check if the reconstructed images match the original images\n",
    "\n",
    "for original_pair, reconstructed_pair in zip(original_images, reconstructed_images):\n",
    "    img_pair_to_compare, annotation_pair_to_compare = zip(original_pair,\n",
    "                                                                                 reconstructed_pair)\n",
    "    print(np.allclose(*img_pair_to_compare))\n",
    "    print(np.allclose(*annotation_pair_to_compare))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import skimage.io as io\n",
    "\n",
    "IMAGE_HEIGHT = 384\n",
    "IMAGE_WIDTH = 384\n",
    "\n",
    "tfrecords_filename = 'pascal_voc_segmentation.tfrecords'\n",
    "\n",
    "def read_and_decode(filename_queue):\n",
    "    reader = tf.TFRecordReader()\n",
    "    \n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "    \n",
    "    features = tf.parse_single_example(\n",
    "        serialized_example,\n",
    "        # Defaults are not specified since both keys are required.\n",
    "        features = {\n",
    "            'height': tf.FixedLenFeature([], tf.int64),\n",
    "            'width': tf.FixedLenFeature([], tf.int64),\n",
    "            'image_raw': tf.FixedLenFeature([], tf.string),\n",
    "            'mask_raw': tf.FixedLenFeature([], tf.string)\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Convert from a scalar string tensor (whose single string has\n",
    "    # length mnist.IMAGE_PIXELS) to a uint8 tensor with shape\n",
    "    # (mnist.IMAGE_PIXELS).\n",
    "    image = tf.decode_raw(features['image_raw'], tf.uint8)\n",
    "    annotation = tf.decode_raw(features['mask_raw'], tf.uint8)\n",
    "    \n",
    "    height = tf.cast(features['height'], tf.int32)\n",
    "    width = tf.cast(features['width'], tf.int32)\n",
    "    \n",
    "    image_shape = tf.pack([height, width, 3])\n",
    "    annotation_shape = tf.pack([height, width, 1])\n",
    "    \n",
    "    image = tf.reshape(image, image_shape)\n",
    "    annotation = tf.reshape(annotation, annotation_shape)\n",
    "    \n",
    "    image_size_const = tf.constant((IMAGE_HEIGHT, IMAGE_WIDTH, 3), dtype=tf.int32)\n",
    "    annotation_size_const = tf.constant((IMAGE_HEIGHT, IMAGE_WIDTH, 1), dtype=tf.int32)\n",
    "    \n",
    "    # Random transformations can be put here: right before you crop images\n",
    "    # to predefined size. To get more information look at the stackoverflow\n",
    "    # question linked above.\n",
    "    \n",
    "    resized_image = tf.image.resize_image_with_crop_or_pad(image=image,\n",
    "                                                          target_height=IMAGE_HEIGHT,\n",
    "                                                          target_width=IMAGE_WIDTH)\n",
    "    resized_annotation = tf.image.resize_image_with_crop_or_pad(image=annotation,\n",
    "                                                               target_height=IMAGE_HEIGHT,\n",
    "                                                               target_width=IMAGE_WIDTH)\n",
    "    \n",
    "    images, annotations = tf.train.shuffle_batch([resized_image, resized_annotation],\n",
    "                                                                batch_size=2,\n",
    "                                                                capacity=30,\n",
    "                                                                num_threads=2,\n",
    "                                                                min_after_dequeue=10)\n",
    "    \n",
    "    return images, annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-2-7ae80093936a>, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-7ae80093936a>\"\u001b[0;36m, line \u001b[0;32m17\u001b[0m\n\u001b[0;31m    for i in xrange(3):\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "filename_queue = tf.train.string_input_producer(\n",
    "    [tfrecords_filename], num_epochs=10)\n",
    "\n",
    "# Even when reading in multiple threads, share the filename queue.\n",
    "image, annotation = read_and_decode(filename_queue)\n",
    "\n",
    "# The op for initializing the variables.\n",
    "init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    \n",
    "    # Let's read off 3 batches just for example\n",
    "    for i in xrange(3):\n",
    "        img, anno = sess.run([image, annotation])\n",
    "        print(img[0, :, :, :].shape)\n",
    "        \n",
    "        print('current batch')\n",
    "        \n",
    "        # We selected the batch size of two\n",
    "        # So we should get two image pairs in each batch\n",
    "        # Let's make sure it is random\n",
    "        io.imshow(img[0, :, :, :])\n",
    "        io.show()\n",
    "        \n",
    "        # 1st segmentation label from batches \n",
    "        io.imshow(anno[0, :, :, :])\n",
    "        io.show()\n",
    "        \n",
    "        io.imshow(img[1, :, :, :])\n",
    "        io.show()\n",
    "        \n",
    "        # 2nd segmentation label from batches \n",
    "        io.imshow(anno[1, :, :, 0])\n",
    "        io.show()\n",
    "        \n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q. Why write the image in bytes/string format, and not int64 or something else?\n",
    "And why do you write height/width to record in int64 then recast to int32 when decoding the record?\n",
    "\n",
    "A. int64_list allows you to serialize integers or a list of integers. However, it will fail if you try to serialize something\n",
    "more complex - e.g. a list of lists (image). Therefore, converting your images into a string is a better choice.\n",
    "The only problem with this approach is restoration process. Effectively, once you convert your\n",
    "string back into integers, you get a list of integers without any shape. However, you can restore\n",
    "the shape, if you know dimensions! Thus, we need to restore the dimensions of the image.\n",
    "tf.reshape takes images dimensions only in tf.in32 format.\n",
    "\n",
    "tf.pack no longer exists in Tensorflow 1.0. Instead, it should be changed to tf.stack.\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
